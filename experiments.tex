\chapter{Experimentation} \label{chap:meth}

\newcolumntype{?}{!{\vrule width 1.5pt}}

 \section{Practical implementation of complex numbers}
Consider a complex number $z=a+ib$ with the real component $a$ and the imaginary component $b$. In $\mathbb{R}$-CNNs, the filter bank or the feature maps of a layer exist in 3 dimensions, where two of them denote the size of the filter or feature map in 2D, and the third dimension indicates how many there are. If we have $N$ feature maps (where $N$ is divisible by 2), the first $N/2$ feature maps would be dedicated to the real components ($a$) and the last $N/2$ feature maps would be dedicated to the imaginary components ($b$). In this manner, the imaginary feature map corresponding to the real feature map on index one of the 3rd dimension would like on the $\frac{N}{2} +1$ position.
   \begin{figure}[htb]
	\centering
	\epsfxsize=7cm
	{\epsfbox{layers.png}}}\caption{Arrangement of $N$ feature maps in a $\mathbb{C}$-CNN layer}
\label{fig:blocks}
\end{figure}


How to write about polar and cartesian form?

\section{Experimentation details}
%For fair comparison of $\mathbb{R}$-CNNs and $\mathbb{C}$-CNNs, we setup conditions as follows.
For experimentation with MNIST+P dataset, we trained using Stochastic Gradient Descent for 64 batch size, 50 epochs, batch 64, and with learning rate 0.001. For the radar datasets, these hyperparameters were the same except for epochs which were 200. The number of starting filters in each stage are 12, 24, and 48, respectively. 

%\begin{table}
\begin{center}
	\captionof{table}{Test accuracy (\%) of activation functions in $\mathbb{C}$-CNNs ($z$ReLU(z), $\mathbb{C}$ReLU(z)), $tanh(z)$), $\mathbb{R}$-CNNs (ReLU) on MNIST+P dataset}
	\begin{tabular}{ c|c|c|c?c } 
		%\hline
		- &$z$ReLU(z) & $\mathbb{C}$ReLU(z) & tanh($z$) & ReLU\\
		\hline Test Accuracy (\%) & 98.31 & 99.07 & 88.42&\textbf{99.59}\\
		%\hline
	\end{tabular}

\end{center}
%\end{table}

Peculiar problem with class with digit 1

\begin{center}
		\captionof{table}{Test accuracy (\%) of activation functions in $\mathbb{C}$-CNNs ($z$ReLU(z), $\mathbb{C}$ReLU(z)), $tanh(z)$) and $\mathbb{R}$-CNNs (ReLU) on Cartesian and Polar representation of Radar-150 and Radar-300 datasets}
	\begin{tabular}{ c|c|c|c?c } 
		%\hline
		- &$z$ReLU(z) & $\mathbb{C}$ReLU(z) & tanh($z$) & ReLU\\
		%\hline Radar-150-Cart (\%) & - & - & - & -\\
		\hline Radar-300-Cart (\%) & 52.76* & 79.09 & NaN & 96.63\\
		\hline Radar-150-Polar (\%) & 70.55 &  90.18& NaN & 97.54 \\
		\hline Radar-300-Polar (\%) & 85.88 & 89.36 & NaN & 92.87\\
		
	\end{tabular}
\end{center}

* 300-cart-zrelu missclassifiying two classes(2,3). no convergence

\section{Discussion}
We DID experience convergence difficulties.
Generalization what?
Perform at par

CONVERGENCE PROBLEMS of tanhz, very data dependent!
CReLU vs ReLU: inly dufference is th convolution - did it owrk? not really...
\subsection{Generalizibility testing experiment}
\subsection{effect of complex weight initialization without batch normalization} 
\subsection{Other architecture}
CReLU and ReLU at least

data augmentation problem class problem. 



 

