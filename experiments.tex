\chapter{Experimentation} \label{chap:meth}

\newcolumntype{?}{!{\vrule width 1.5pt}}

 \section{Practical implementation of complex numbers}
Consider a complex number $z=a+ib$ with the real component $a$ and the imaginary component $b$. In $\mathbb{R}$-CNNs, the filter bank or the feature maps of a layer exist in 3 dimensions, where two of them denote the size of the filter or feature map in 2D, and the third dimension indicates how many there are. If we have $N$ feature maps (where $N$ is divisible by 2), the first $N/2$ feature maps would be dedicated to the real components ($a$) and the last $N/2$ feature maps would be dedicated to the imaginary components ($b$). In this manner, the imaginary feature map corresponding to the real feature map on index one of the 3rd dimension would like on the $\frac{N}{2} +1$ position.

IMAGE REQUIRED



How to write about polar and cartesian form?


\subsection{Data Analysis}


\section{Results}

%\begin{table}
\begin{center}
	\captionof{table}{Test accuracy (\%) of activation functions in $\mathbb{C}$-CNNs ($z$ReLU(z), $\mathbb{C}$ReLU(z)), $tanh(z)$), $\mathbb{R}$-CNNs (ReLU) on MNIST+P dataset}
	\begin{tabular}{ c|c|c|c?c } 
		%\hline
		- &$z$ReLU(z) & $\mathbb{C}$ReLU(z) & tanh($z$) & ReLU\\
		\hline Test Accuracy (\%) & 98.31 & 99.07 & 88.42&\textbf{99.59}\\
		%\hline
	\end{tabular}

\end{center}
%\end{table}

Peculiar problem with class with digit 1

\begin{center}
		\captionof{table}{Test accuracy (\%) of activation functions in $\mathbb{C}$-CNNs ($z$ReLU(z), $\mathbb{C}$ReLU(z)), $tanh(z)$) and $\mathbb{R}$-CNNs (ReLU) on Cartesian and Polar representation of Radar-150 and Radar-300 datasets}
	\begin{tabular}{ c|c|c|c?c } 
		%\hline
		- &$z$ReLU(z) & $\mathbb{C}$ReLU(z) & tanh($z$) & ReLU\\
		%\hline Radar-150-Cart (\%) & - & - & - & -\\
		\hline Radar-300-Cart (\%) & 52.76* & 79.09 & NaN & 96.63\\
		\hline Radar-150-Polar (\%) & - &  90.18& NaN & 97.54 \\
		\hline Radar-300-Polar (\%) & - & 89.36 & - & 92.87\\
		
	\end{tabular}
\end{center}

* 300-cart-zrelu missclassifiying two classes(2,3). no convergence

\section{Discussion}
We DID experience convergence difficulties.
Generalization what?
Perform at par

CONVERGENCE PROBLEMS of tanhz, very data dependent!
CReLU vs ReLU: inly dufference is th convolution - did it owrk? not really...
\subsection{Generalizibility testing experiment}
\subsection{effect of complex weight initialization without batch normalization} 
\subsection{Other architecture}
CReLU and ReLU at least
 

