\chapter{Comparison of $\mathbb{Re}$-CNNs with $\mathbb{C}$-CNNs}\label{chap:meth}
 
 
 \section{Convolutional Neural Networks}
 
 Convolutional Neural Networks (CNNs) (LeCun, 1989) \cite{lecun1989backpropagation} are specialized neural networks that are used for processing data that can be represented in the form of a grid or arrays e.g. 1D arrays of audio signals, three channels of 2D arrays of RGB images, or any other higher dimensional data. CNNs take their name due to the presence of a specialized kind of linear operation called convolution. CNNs are inspired by the visual cortex in the brain, which consists of alternating layers of simple and complex cells  (Hubel \& Wiesel, 1959, 1962) \cite{hubel1959receptive}\cite{hubel1962receptive}. 
 
In the context of image classification, CNNs make use of the inherent compositional hierarchies present in images such that higher-level features are obtained by composing lower-level ones. Additionally, there are three key ideas behind ConvNets that makes them attractive for learning: sparse interactions, shared weights, equivariant representations. In traditional neural netowrks, one learned parameter interacts with the one input unit in order to produce an output. In the case of images where there are millions of pixels, this approach becomes extremely expensive. In this respect, CNNs offer sparse interactions between inputs and the ouputs in the sense that very few parameters of the kernels need to be learned to extract features from an input of, let's say, a million pixels. This reduces the memory requirements and improve statistical effeciency. Parameter sharing referes to the use of the same parameters for than one function in the model. When kernels are learned in a CNN, the same kernel is used for different spatial locations of the input rather than learning different parameters for different spatial locations. This further reduces the memory requirements. CNNs also grant equivariance to translations, meaning that if the presence of a feature translates in an image, the output of a convolutional layer will also move by the same amount. This behavior is useful if we want to detect the same feature in different parts of the image.\cite{DeepLearning}
 
 In the following section, the building blocks of a real-valued CNN ($\mathbb{Re}$-CNN) and its complex counterpart, complex-valued CNN ($\mathbb{C}$-CNN), pertinent to our methodology, are discussed in detail.
 
 \section{The Building Blocks}
 
 
 
 \subsection{Convolution}
  In the field of Computer Vision, convolution operation is utilized for applying a 2D filter to an image for the purpose of photo enhancement or feature extraction. In photo enhancement, the filters used can have different effects e.g. sharpening, blurring, dilatting, and more. In case of features extraction, different features i.e. edges, corners, gradients etc. can be extracted. In short, the application of filters through convolution operation enables us to convert images in a form which are easier to understand and can further help in myriads of computer vision applications such as face detection, object detection, SLAM etc. 
  
  In a CNN, the convolutional layer performs the role of feature extraction in a way that each succeeding layer represents a more complex concept that helps get closer to classification \cite{nature}. For examples, the first layers would learn low-level features (e.g. edges) form patterns and the latter layers would learn higher level features (e.g. shapes, textures) which would inturn be composed of these the earlier ones. 

 
 Units in a convolutional layer are organized in feature maps, within which each unit is connected to local patches in the feature maps of the previous  layer through a set of weights called a filter bank \cite{nature}.  The local patches in the feature maps of the previous layer are called the receptive field for the respective unit in the feature map under discussion. Different feature maps in a layer use different filter banks but each unit in the same feature map undergo the same filter bank. The new feature map can be obtained by first convolving the input with a learned kernel and then applying an element-wise nonlinear activation function on the convolved results\cite{recent_advances}. The nonlinear acitvation function will be discussed in detail in Section \ref{ssec:act_fn}. 
 
 NUMBER OF PARAMETERS/ MORE ACTIVATION FUNCTIONS IN POLSARHAENCSH
 
 \subsubsection{Real-Valued Convolution}
 Convolution entails point-wise multiplication of the filter with the input image and summing all the multiplicands to form the unit in the feature map corresponding to that on which the filter's central unit is positioned in the output feature map (after the application of non-linear activation function). The filter slides and performs this operation all over the space of the input by centering upon all or some of the units of the input, according to the design choice of number of strides. The complete feature maps are obtained by using several different kernels. \\
 
 Mathematically, the feature value at location $(i, j)$ in the k-th feature map of $l$-th layer, $z_{i,j,k}^l$, is calculated by:
 \begin{equation}
 z_{i,j,k}^l= {w_{k}^l}^T*x^l_{i,j} + b^l_k
 \end{equation}
 
 
 where $w_k^{l}$ and $b_k^{l}$ are the weight vector and bias term of the $k$-th filter of the $l$-th layer respectively, and
 $x^l_{i,j}$ is the input patch centered at location $(i, j)$ of the $l$-th layer. Note that the kernel $w_k^{l}$ that generates
 the feature map $z^l_{:,:,k}$ is shared. Such a weight sharing mechanism has several advantages such as it can reduce the model complexity and make the network easier to train\cite{recent_advances}. 
 
\begin{figure}[htb]
	\centering
	\epsfxsize=6cm
	{\epsfbox{convRiverTrailBlog}}
	\caption{A schematic sketch of the convolution operation. An unit in the output is the sum of point-wise multiplication of the kernel and input patch \cite{bloggg}}
	\label{fig:conv}
\end{figure}
 

\subsubsection{Complex-Valued Convolution}
Like complex domain can be thought of as an extension of real domain, complex-valued convolution can also be formulated through such thinking. In complex-valued convolution, the filter and the input both take on the complex form:
 \begin{equation}
\mathrm{W} = \mathrm{A}+i\mathrm{B}
\label{eqCfilter}
\end{equation}
 \begin{equation}
\mathrm{h} = \mathrm{x}+i\mathrm{y}
\label{eqCinput}
\end{equation}
 
where A and B are real-valued matrices which belong to one complex kernel, implemented as seperate layers. x and y are real-valued input vecetors  representing the real and imaginary part of the complex-valued patch of the input layer. Note that the real and imaginary parts of both the kernel and the input are present as separate layers, and the result of the convolution is one complex-valued layer represented as two real-valued layers \cite{Guberman}. 

Taking advantage of the fact that convolution is distributive, the result of $\mathrm{W*h}$ results as follows:

 \begin{equation}
\mathrm{W*h} = (\mathrm{A*x-B*y})+i(\mathrm{B*x+A*Y})
\label{eqCconv}
\end{equation}


This convolution strategy has been used in \cite{Guberman}, \cite{trabelsi2018deep} , Learning Rep, Polsar and ...  
 \subsection{Batch Normalization}\label{ssec:bn}
One of the most widespread methods used to improve the performance of Deep Neural networks, including CNNs, is called Batch Normalization (BN). 
%Problem faced, how it is solved: 
Introduced by Ioffe and Szegedy in 2015 \cite{bnIoffeS15}, BN helps speed up the training of the network and enables the use of a wider variety of acceptable hyperparameters by reducing internal covariant shift. 
Internal Covariate Shift is defined as the change in the distribution of network activations, the input to the next layer, due to the change in network parameters during training \cite{bnIoffeS15}. Normalization of input data is widely accepted method to make the nerual networks converge faster, as stated in \cite{LeCun1998}. In the case of BN, this idea of input normalization is applied on the input of each layer which happens to be the output of the previous layer. It fights the internal covariate shift problem by a normalization step that fixes the means and variances of layer inputs where the estimations of mean and variance are computed
after each mini-batch rather than the entire training set \cite{recent_advances}. The exact procedure proposed is described lates in \ref{rvbn}.

%Advantages
Batch normalization has many advantages compared with global data
normalization that is done before training. Apart from reducing internal covariant shift, BN also reduces the dependence of gradients on the scale of the parameters or of their initialization strategy. This benefits the flow of gradients and enables the use of higher learning rate without the risk of divergence \cite{recent_advances}. BN also weakens the coupling between functions of each layers which inturn expedites convergence.  
Furthermore, as BN involves subtraction of mean and scaling by standard deviation, where mena and standard deviation are calculated on mini-batches, multiplicative and additive noise is added to the each iteration. As a result, a slight regularization effect is induced in the network which helps it not to rely too much on the output of a single neuron/node in any layer of the network. One should note that the bigger the size of the mini-batch, the weaker is the regularization effect. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%At test time????

 \subsubsection{Real-Valued Batch Normalization}\label{rvbn}
 %Exact procedure of R goes here
In $\mathbb{Re}$-CNNs, the BN step is generally performed between the convolution step and the activation step, although the original paper \cite{bnIoffeS15} proposed it for after the activation step. Suppose that a layer to normalize has a $d$ dimensional
input, i.e., $\mathrm{x} = [x_{1} , x_{2} , ..., x_{d} ]^{\mathrm{T}}$ .  The first step is to normalize the $k$-th dimension as follows:


 \begin{equation}
\hat{x}_{k} = \frac{x_{k} - \mu}{\sqrt{\sigma^{2} - \epsilon}}
\label{eqrbn1}
\end{equation}

where $\mu$ and $\sigma^{2}$ are the mean and variance of mini-batch respectively, and $\epsilon$ is a constant value. Next, we let the network change the distribution of the network as is required by the model throught he help of one multiplicative and one additive paramter. The normalized input $\hat{x}_{k}$ is further transformed into:
 \begin{equation}
y_{k} = \mathrm{BN}_{\gamma,\beta}(x_{k}) = \gamma\hat{x}_{k} + \beta
\label{eqrbn2}
\end{equation}
where $\gamma$ and $\beta$ are learned parameters. It might not be beneficial for the layer input to have zero mean and variance, although the network has the choice to keep them such through the two parameters. $\gamma$ and $\beta$ help scale and shift the mean of the input distribution to better make advantage of the nonlinearity of the activation function.


 \subsubsection{Complex-Valued Batch Normalization}
 %Exact procedure of C from DCN goes here; mention that it was first introduced in this paper
The complex-valued BN is an extension of its real-valued counterpart and it was first introduced in \cite{trabelsi2018deep}. The first step, ofcourse, is normalization of the input distribution. Unlike real-valued BN, merely translating and scaling, like in \cite{bnIoffeS15}, would result in skewed or elliptical variance with high eccentricity \cite{trabelsi2018deep}. To cater for this problem, the authors the normalization step of complex BN is treated  as whitening of 2D vectors. The following equations describe the process.

 \begin{equation}
\tilde{x} = (V^{-\frac{1}{2}})(x - \mathbb{E}[x])
\label{eqcbn1}
\end{equation}

 \begin{equation}
V = \begin{pmatrix} V_{rr} & V_{ri}\\ V_{ir} & V_{ii} \end{pmatrix} = 
\begin{pmatrix} \mathrm{Cov}(\Re{\{x\}},\Re{\{x\}}) & \mathrm{Cov}(\Re{\{x\}},\Im{\{x\}})\\  \mathrm{Cov}(\Im{\{x\}},\Re{\{x\}}) & \mathrm{Cov}(\Im{\{x\}},\Im{\{x\}}) \end{pmatrix}
\label{eqcbn2}
\end{equation}

As described by the equations above, the mean-centered input ($x - \mathbb{E}[x]$) is multiplied by the inverse of the square-root of input variance $\mathrm{V}$ in the normalizatios step.%The guarantee of positive definiteness of $\mathrm{V} 
Whereas in real-valued BN, the first step entailed converting the input distribution into a standard normal distribution, the complex-valued BN requires it to be converted to standard complex normal ditribution, which is characterized by having location parameter (also called mean) $\mu = 0$, covariance matrix $\Gamma = 1$, and the relation matrix (also called pseudo-covariance) $C = 0$. The formulae of these parameters are given by:

\begin{equation}
\label{eqcbn3}
\begin{aligned}
&\mu = \mathbb{E}[\tilde{x}]&\\
&\Gamma = \mathbb{E}[(\tilde{x}-\mu)(\tilde{x}-\mu)^{*}] = V_{rr} + V_{ii} + i (V_{ir} - V_{ri} )&\\
&C = \mathbb{E}[(\tilde{x}-\mu)(\tilde{x}-\mu)] = V_{rr} - V_{ii} + i (V_{ir} + V_{ri} )&
\end{aligned}
\end{equation}

The next step in complex-valued BN is to scale and shift the input distrubtion to a desired mean and variance, with the help of $\gamma$ and $\beta$, respectively. $\gamma$ is now a $\mathrm{2} \mathrm{x} \mathrm{2}$ positive-semidefinite matrix with three tunable parameters and it is given by:

\begin{equation}
\gamma = \begin{pmatrix} \gamma_{rr} & \gamma_{ri}\\ \gamma_{ri} & \gamma_{ii} \end{pmatrix} 
\label{eqcbn4}
\end{equation}

$\beta$ is a complex parameter with real and imaginary learnable components. The final step of complex-valued BN follows the same equation as of real-valued BN, given by:

 \begin{equation}
y_{k} = \mathrm{BN}_{\gamma,\beta}(x_{k}) = \gamma\hat{x}_{k} + \beta
\label{eqcbn5}
\end{equation}

%SKIPPING: DECORRELATION EFFECT, IMPLEMENTATION DETAILS....
 \subsection{Activation Function}\label{ssec:act_fn}
 In the absence of non-linear activation functions, the output of a neural network would essentially be a linear combination of the input, however deep the network is. Non-linear activation functions are introduced in order to add non-linearity to the network so that functions of a higher degree than 1 can be learned, thuse giving them the title of universal function approximators. In a CNN, activation function is applied element-wise to the output of the convolution (or BN, when it is used) and they act as to decide whether or not this output should be taken into account by its connection, and if so, what should be the nature of it. For $\mathrm{R}$-CNNs, some common types of activation functions include Sigmoid, Hyperbolic Tangent function (tanh), and the most popular, Recitified Linear Unit (ReLU).
 
 %They also serve as to provide a degree of robustness to data variability and %a computationally efficient representation, courtesy of sparsity induction.  
 
  \subsubsection{Real-Valued Activation Functions}
   \subsubsection{ReLU}
   
Popularized by \cite{krizhevsky2012imagenet}, Rectified Linear Units (ReLU) \cite{nair2010rectified} remains the most commonly used activation function in deep learning/CNN architectures for the community. ReLU is given by:

\begin{equation}
f(x) =  
\begin{cases} 
x , &\textrm{for} \ x\ge 0\\
0 , &\textrm{for} \ x < 0
\end{cases}
\end{equation}
 
%benefits of ReLU
%other 
 
 Other variants of ReLU include Leaky ReLU (LReLU), Parametric ReLU (PReLU), Randomized ReLU (RReLU), and Exponential Linear Unit (ELU), as shown in figure blaa blaa. 
  
 \subsubsection{Complex-Valued Activation Functions}
 
 Real-valued activation functions have to strictly comply with the requirement of differentiability for the chain rule, the backbone of the Gradient Descent optimization algorithm, to work. Since $\mathbb{C}$-CNNs also make use of the 
 same optimization algorithm, it is only natural to think that the complex activation functions we require must also be differentiable at any point in their domain i.e. they should be Holomorphic. However, Louiville theorem presents us with a realization: the only complex functions that can be  Holomorphic are constant functions. This begs us to question if the condition of differentiability can be relaxed while still being able to optimize the network.
 We find a way to circumvent this problem by ignoring the requirement complex differentiablity, finding solace in the fact that as long as the complex function is differentiable with respect to its real part and its imaginary part, it provides sufficiency to be able to perform backpropagation, as demonstrated in \cite{trabelsi2018deep}, \cite{hansch2010complex}, \cite{polsarzhang2017complex}. The chain-rule for complex function differentiation is given in section \ref{cchainrule}. However, we must note that since Holomorphic functions obey CR equations, they are beneficial grant computational efficiency to our process because we would only need to compute half the number of partial derivatives \cite{sarroff2015learning}.
 
 %cooridnate dependency????
     
 Complex-valued activation functions can be divided into two types: Real-Imaginary type , and the Amplitude-Phase type \cite{hirose2012complex}.
 The first type of activation functions deals with real and imaginary parts separately and independently, but identically. Hence, they can be thought to work similar to real-valued networks with double the number of dimensions.  The latter type applies the nonlinearity on only the magnitude of the complex number, leaving the phase information unchanged \cite{page45hirose2012complex} . 
 
 %suitability of the two types functions???
 
 
 %" Instead, we construct the dynamics of learning and self-organization on the basis of meaningful partial derivatives in complex domain with deliberation on the properties of  input and output information "
 

 \subsubsection{modReLU}
Arjovsky $et \ al.$ (2015) \cite{ArjovskySB15} proposed modReLU, an Amplitude-Phase type complex activation function, as a possible activation function for complex valued networks. It is formulated as:
 \begin{equation}
\mathrm{modReLU}(z) = \mathrm{ReLU}(|z| + b)e^{i\theta_{z}}
{}=\begin{cases} (|z| + b)\frac{z}{|z|} , &\mathrm{for} \ |z| + b\ge 0 \\ 0 , &\textrm{otherwise} \end{cases}
\end{equation}

where $b$ is a learnable parameter which dictates the offset for the deadzone around the origin $0$. modReLU is an element-wise nonlinearity, where the element is muted(assigned a value of zero in both real and imaginary channels) if it lies in the deadzone, or passed if it is beyond. Chiheb $et \ al.$ \cite{trabelsi2018deep} also make use of this non-linearity in their analysis of $\mathbb{R}$-CNNs and $\mathbb{C}$-CNNs.


\subsubsection{$\mathbb{C}$ReLU}
$\mathbb{C}$ReLU is Real-Imaginary type activation function introduced by Chiheb $et \ al.$ (2018) \cite{trabelsi2018deep}, which satisfies the CR equations only in the first and the third quadrant. The formula is given as follows:

\begin{equation}
\label{eqcrelu}
\mathbb{C}\mathrm{ReLU}(z) = \mathrm{ReLU}(\Re(z))+i\mathrm{ReLU}(\Im(z))
\end{equation}

 
 
 \subsubsection{$z$ReLU}
 Proposed by Guberman (2106) \cite{Guberman}, $z$ReLU does not strictly fall in any of the two categories described above. It follows the CR equations on all points on the complex domain except for the points along the real and imaginary axes of the complex plane. It is defined as follows:
  \begin{equation}
  \label{eqzrelu}
 z\mathrm{ReLU}(z) 
 {}=\begin{cases} z , &\mathrm{for} \ \theta_{z} \in [0,\pi/2] + b\ge 0 \\ 0 , &\textrm{otherwise} \end{cases}
 \end{equation}

 \subsection{Weight Initialization}
 Weight initialization can help in speeding up the convergence of the gradient descent algorithm, and combats the problem of vanishing and exploding gradients, especially in the absence of BN. 
 
 
 \subsubsection{Real-Valued Weight Initialization}\label{rvwi}
 In the context of $\mathbb{R}$-CNNs, two weight initialization techniques are popular among the community, which are explained below. In these two techniques, the first step concerns with choosing choosing a random distribution function whose defining parameters can be adjusted according to their benefits.
 \paragraph{Glorot and Bengio (2010) criterion}
 This type of initialization demands that the variance of the weight distibution should be as follows:

  \begin{equation}\label{gbwi}
\mathrm{Var}(W) = 2/(n_{in}+n_{out})
\end{equation}
 
 The constraint ensures that the variances of the input, output and the gradients are the same. They go ahead to pick the weights from a Gaussian distribution with zero mean and variance as given in equation\ref{gbwi}. This initialization has been proved to be useful owing to the fact that it can detect the scale of initialization based on the number of inputs and outputs, keeping the signal in a reasonable limit through many layers \cite{recent_advances} \cite{glorot2010understanding}.
 
 
 \paragraph{He $et \ al.$ (2015b) criterion}
 He $et \ al.$ (2105b) proposed an initialization criterion specifically accounts for ReLU non-linearity function, which grants the ability to train extremely deep networks \cite{recent_advances} \cite{he2015delving}. 
 
 
 
 \begin{equation}\label{hewi}
 \mathrm{Var}(W) = 2/n_{in}
 \end{equation}
 \subsubsection{Complex-Valued Weight Initialization}
 Chiheb $et \ al.$ (2018) introduced two methods of complex weight initialization, which incorporate the two real-valued $\mathbb{R}$-CNN weight initialization techniques given above.
 
 They start with dealing with the problem of defining the variance of complex weights. The standard definition of variance of complex variables goes as follows:
 
 \begin{equation}\label{vardef}
 \mathrm{Var}(W) = \mathbb{E}[WW^{*}]-(\mathbb{E}[W])^2 = \mathbb{E}[|W|^{2}]-(\mathbb{E}[W])^2
 \end{equation}
 
 
 When $W$ is a symmtrically distributed around 0, equation \ref{vardef} reduces to $\mathrm{Var}(W) = \mathbb{E}[|W|^{2}]$. In order to calculate this variance, we adopt a route that involves Rayleigh distribution. We know that the variance of the magnitude of complex weights, $\mathrm{Var}(|W|)$, follows a Rayleigh distribution, given below:
 
 \begin{equation}\label{varmagrayleighdef}
 \mathrm{Var}(|W|) = \mathbb{E}[|W||W|^{*}]-(\mathbb{E}[|W|])^2 = \mathbb{E}[|W|^{2}]-(\mathbb{E}[|W|])^2
 \end{equation}
 
 Bu substitution, we reach the following formulation for the variance of the weights:
 
 \begin{equation}\label{varw2}
 \mathrm{Var}(|W|) = \mathrm{Var}(W)-(\mathrm{E}[|W|])^2 , and  
 \mathrm{Var}(W) = \mathrm{Var}(|W|) + (\mathrm{E}[|W|])^2
 \end{equation}
  
 The expectation and variance of magnitude of complex weights can be computer using the definitions given by the Rayleigh distribution, which can be characterized by just one parameter, mode ($\sigma$). By substitution, we can finally define the variance of the complex weights.
 
 \begin{equation}\label{ray}
 \mathrm{E}(|W|) = \sigma\sqrt{\frac{\pi}{2}},\\  
 \mathrm{Var}(|W|) = \frac{4-\pi}{2}\sigma^{2}
 \end{equation}
 
 
 \begin{equation}\label{ray}
 \mathrm{Var}(W) = 2\sigma^2
 \end{equation}
 
 Now that the variance is defined,  we can choose either Glorot and Bengio criterion or the He. $2t \ al.$ (2015b) criteron to equate to define the vaue of $\sigma$. In the former criterion, we get $\sigma = 1/\sqrt{n_{in}+n_{out}}$, and in the latter, $\sigma = 1/\sqrt{n_{in}}$. Having the appropriate $\sigma$, we can pick the magnitude of our weights from teh Rayleigh distribution. However, as the variance of $W$ depends only on the magnitude of the weights and not their phase,  we have the liberty of choosing phase. In their implementation, phase is chosen from a uniform distribution of -$\pi$ to $\pi$.
 
 %\subsubsection{Complex-Valued Weight Initialization}
 
 
 \paragraph{Complex Weight Initialization}
 %\paragraph{Complex Independent Weight Initialization}
 
 \subsection{Output Layer and Loss Function}
 
 \subsubsection{Real-Valued CNNs}
 \subsubsection{Complex-Valued CNNs}
 
 \subsection{Optimization through Backpropagation}









