\chapter{Comparison of $\mathbb{Re}$-CNNs with $\mathbb{C}$-CNNs}\label{chap:meth}
 
 
 \section{Convolutional neural networks}\label{cnn41}
 
 Convolutional Neural Networks (CNNs) (LeCun, 1989) \cite{lecun1989backpropagation} are specialized neural networks that are used for processing data that can be represented in the form of a grid or arrays e.g. 1D arrays of audio signals, three channels of 2D arrays of RGB images, or any other higher dimensional data. CNNs take their name due to the presence of a specialized kind of linear operation called convolution. CNNs are inspired by the visual cortex in the brain, which consists of alternating layers of simple and complex cells  (Hubel \& Wiesel, 1959, 1962) \cite{hubel1959receptive}\cite{hubel1962receptive}. 
 
In the context of image classification, CNNs make use of the inherent compositional hierarchies present in images such that higher-level features are obtained by composing lower-level ones. Additionally, there are three key ideas behind ConvNets that makes them attractive for learning: sparse interactions, shared weights, and equivariant representations \cite{DeepLearning}. In traditional neural networks, one learned parameter interacts with one input unit in order to produce an output. In the case of images where there are millions of pixels, this approach becomes extremely expensive. In this respect, CNNs offer sparse interactions between inputs and the outputs in the sense that kernels of very few parameters need to be learned to extract features from an input of, let's say, a million pixels. This reduces the memory requirements and improve statistical efficiency. Parameter sharing refers to the use of the same parameters for than one function in the model. When kernels are learned in a CNN, the same kernel is used for different spatial locations of the input rather than learning different parameters for different spatial locations. This further reduces the memory requirements. CNNs also grant equivariance to translations, meaning that if the presence of a feature translates in an image, the output of a convolutional layer will also move by the same amount. This behavior is useful if we want to detect the same feature in different parts of the image. An example of a CNN, provided in LeCun (2015) paper \cite{nature}, is given in Figure \ref{fig:cnn}.

\begin{figure}[htb]
	\centering
	\epsfxsize=15cm
	{\epsfbox{cnn}}
	\caption{The outputs of each layer of a CNN can be seen as applied to a picture of Samoyed Dog. The RGB channels are fed into the network, and is met by two instances of Convolution, Rectified Linear Unit (RelU), and Max-pooling layer before reaching the output layer. Each image is a feature map corresponding to the output of the learned features at each image position. The output layer provides the probabilities of the image being a specific class. \cite{bloggg}}
	\label{fig:cnn}
\end{figure}


 
 In the following section, the building blocks of a real-valued CNN ($\mathbb{Re}$-CNN) and its complex counterpart, complex-valued CNN ($\mathbb{C}$-CNN), pertinent to our experiments, are discussed in detail.
 
 \section{The building blocks of $\mathbb{R}$-CNNs and $\mathbb{C}$-CNNs}
 
 
 
 \subsection{Convolution}\label{conv}
  In the field of Computer Vision, convolution operation is utilized for applying a 2D filter to an image for the purpose of photo enhancement or feature extraction. In photo enhancement, the filters used can have different effects such as sharpening, blurring, dilating, and more, whereas in feature extraction, different features such as edges, corners, gradients etc. can be extracted. In short, the application of filters through convolution operation enables us to convert images in a form which are easier to understand and can further help in myriads of computer vision applications such as face detection, object detection, SLAM etc. 
  
  In a CNN, the convolutional layer performs the role of feature extraction in a way that each succeeding layer represents a more complex concept that helps get closer to classification \cite{nature}. For examples, the first layers would learn low-level features (e.g. edges) form patterns and the latter layers would learn higher level features (e.g. shapes, textures) which would inturn be composed of these the earlier ones. 

 
 Units in a convolutional layer are organized in feature maps, within which each unit is connected to local patches in the feature maps of the previous  layer through a set of weights called a filter bank \cite{nature}.  The local patches in the feature maps of the previous layer are called the receptive field for the respective unit in the feature map under discussion. Different feature maps in a layer use different filter banks but each unit in the same feature map undergo the same filter bank. The new feature map can be obtained by first convolving the input with a learned kernel and then applying an element-wise nonlinear activation function on the convolved results\cite{recent_advances}. The nonlinear acitvation function will be discussed in detail in Section \ref{ssec:act_fn}. 
 
 %NUMBER OF PARAMETERS/ MORE ACTIVATION FUNCTIONS IN POLSARHAENCSH
 
 \subsubsection{Convolution in $\mathbb{R}$-CNNs}
 Convolution entails point-wise multiplication of the filter with the input image and summing all the multiplicands to form the unit in the feature map corresponding to that on which the filter's central unit is positioned in the output feature map (after the application of non-linear activation function). The filter slides and performs this operation all over the space of the input by centering upon all or some of the units of the input, according to the design choice of number of strides. The complete feature maps are obtained by using several different kernels. \\
 
 Mathematically, the feature value at location $(i, j)$ in the k-th feature map of $l$-th layer, $z_{i,j,k}^l$, is calculated by:
 \begin{equation}
 z_{i,j,k}^l= {w_{k}^l}^T*x^l_{i,j} + b^l_k
 \end{equation}
 
 
 where $w_k^{l}$ and $b_k^{l}$ are the weight vector and bias term of the $k$-th filter of the $l$-th layer respectively, and
 $x^l_{i,j}$ is the input patch centered at location $(i, j)$ of the $l$-th layer. Note that the kernel $w_k^{l}$ that generates
 the feature map $z^l_{:,:,k}$ is shared. Such a weight sharing mechanism has several advantages such as it can reduce the model complexity and make the network easier to train\cite{recent_advances}. 
 
\begin{figure}[htb]
	\centering
	\epsfxsize=6cm
	{\epsfbox{convRiverTrailBlog}}
	\caption{A schematic sketch of the convolution operation. An unit in the output is the sum of point-wise multiplication of the kernel and input patch \cite{bloggg}}
	\label{fig:conv}
\end{figure}
 

\subsubsection{Convolution in $\mathbb{C}$-CNNs}
Like complex domain can be thought of as an extension of real domain, complex-valued convolution can also be formulated through such thinking. In complex-valued convolution, the filter and the input both take on the complex form:
 \begin{equation}
\mathrm{W} = \mathrm{A}+i\mathrm{B}
\label{eqCfilter}
\end{equation}
 \begin{equation}
\mathrm{h} = \mathrm{x}+i\mathrm{y}
\label{eqCinput}
\end{equation}
 
where A and B are real-valued matrices which belong to one complex kernel, implemented as seperate layers. x and y are real-valued input vecetors  representing the real and imaginary part of the complex-valued patch of the input layer. Note that the real and imaginary parts of both the kernel and the input are present as separate layers, and the result of the convolution is one complex-valued layer represented as two real-valued layers \cite{Guberman}. 

Taking advantage of the fact that convolution is distributive, the result of $\mathrm{W*h}$ is given in equation \ref{eqCconv}. A schematic of the implementation of convolution in our experiments is depicted in \ref{fig:complexconv}.

 \begin{equation}
\mathrm{W*h} = (\mathrm{A*x-B*y})+i(\mathrm{B*x+A*Y})
\label{eqCconv}
\end{equation}

 \begin{figure}[htb]
	\centering
	\epsfxsize=10cm
	{\epsfbox{cConvOperation}}
	\caption{A schematic sketch of the complex-convolution operation. M and K represent the feature map and kernel, respectively.  \cite{trabelsi2018deep}}
	\label{fig:complexconv}
\end{figure}

%This convolution strategy has been used in \cite{Guberman}, \cite{trabelsi2018deep} , Learning Rep, Polsar and ...  
 \subsection{Batch normalization}\label{ssec:bn}
One of the most widespread methods used to improve the performance of Deep Neural networks, including CNNs, is called Batch Normalization (BN). 
%Problem faced, how it is solved: 
Introduced by Ioffe and Szegedy in 2015 \cite{bnIoffeS15}, BN helps speed up the training of the network and enables the use of a wider variety of acceptable hyper-parameters by reducing internal covariate shift. 
Internal Covariate Shift is defined as the change in the distribution of network activations, the input to the next layer, due to the change in network parameters during training \cite{bnIoffeS15}. Normalization of input data is widely accepted method to make the nerual networks converge faster, as stated in \cite{LeCun1998}. In the case of BN, this idea of input normalization is applied on the input of each layer which happens to be the output of the previous layer. It fights the internal covariate shift problem by a normalization step that fixes the means and variances of layer inputs where the estimations of mean and variance are computed
after each mini-batch rather than the entire training set \cite{recent_advances}. The exact procedure proposed is described later in \ref{rvbn}.

%Advantages
Batch normalization has many advantages compared with global data
normalization that is done before training. Apart from reducing internal covariant shift, BN also reduces the dependence of gradients on the scale of the parameters or of their initialization strategy. This benefits the flow of gradients and enables the use of higher learning rate without the risk of divergence \cite{recent_advances}. BN also weakens the coupling between functions of each layers which in turn expedites convergence.  
Furthermore, as BN involves subtraction of mean and scaling by standard deviation, where mean and standard deviation are calculated on mini-batches, multiplicative and additive noise is added to the each iteration. As a result, a slight regularization effect is induced in the network which helps it not to rely too much on the output of a single neuron/node in any layer of the network. One should note that the bigger the size of the mini-batch, the weaker is the regularization effect. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%At test time????

 \subsubsection{Real-valued batch normalization}\label{rvbn}
 %Exact procedure of R goes here
In $\mathbb{Re}$-CNNs, the BN step is generally performed between the convolution step and the activation step, although the original paper \cite{bnIoffeS15} proposed it for after the activation step. Suppose that a layer to normalize has a $d$ dimensional
input, i.e., $\mathrm{x} = [x_{1} , x_{2} , ..., x_{d} ]^{\mathrm{T}}$ .  The first step is to normalize the $k$-th dimension as follows:


 \begin{equation}
\hat{x}_{k} = \frac{x_{k} - \mu}{\sqrt{\sigma^{2} - \epsilon}}
\label{eqrbn1}
\end{equation}

where $\mu$ and $\sigma^{2}$ are the mean and variance of mini-batch respectively, and $\epsilon$ is a constant value. Next, we let the network change the distribution of the network as is required by the model through the help of one multiplicative and one additive parameter. The normalized input $\hat{x}_{k}$ is further transformed into:
 \begin{equation}
y_{k} = \mathrm{BN}_{\gamma,\beta}(x_{k}) = \gamma\hat{x}_{k} + \beta
\label{eqrbn2}
\end{equation}
where $\gamma$ and $\beta$ are learned parameters. It might not be beneficial for the layer input to have zero mean and variance, although the network has the choice to keep them such through the two parameters. $\gamma$ and $\beta$ help scale and shift the mean of the input distribution to better make advantage of the nonlinearity of the activation function.


 \subsubsection{Complex-valued batch normalization}
 %Exact procedure of C from DCN goes here; mention that it was first introduced in this paper
The complex-valued BN is an extension of its real-valued counterpart and it was first introduced in \cite{trabelsi2018deep}. The first step, of course, is normalization of the input distribution. Unlike real-valued BN, merely translating and scaling, like in \cite{bnIoffeS15}, would result in skewed or elliptical variance with high eccentricity \cite{trabelsi2018deep}. To cater for this problem, the authors the normalization step of complex BN is treated  as whitening of 2D vectors. The following equations describe the process.

 \begin{equation}
\tilde{x} = (V^{-\frac{1}{2}})(x - \mathbb{E}[x])
\label{eqcbn1}
\end{equation}

 \begin{equation}
V = \begin{pmatrix} V_{rr} & V_{ri}\\ V_{ir} & V_{ii} \end{pmatrix} = 
\begin{pmatrix} \mathrm{Cov}(\Re{\{x\}},\Re{\{x\}}) & \mathrm{Cov}(\Re{\{x\}},\Im{\{x\}})\\  \mathrm{Cov}(\Im{\{x\}},\Re{\{x\}}) & \mathrm{Cov}(\Im{\{x\}},\Im{\{x\}}) \end{pmatrix}
\label{eqcbn2}
\end{equation}

As described by the equations above, the mean-centered input ($x - \mathbb{E}[x]$) is multiplied by the inverse of the square-root of input variance $\mathrm{V}$ in the normalization step.%The guarantee of positive definiteness of $\mathrm{V} 
Whereas in real-valued BN, the first step entailed converting the input distribution into a standard normal distribution, the complex-valued BN requires it to be converted to standard complex normal distribution, which is characterized by having location parameter (also called mean) $\mu = 0$, covariance matrix $\Gamma = 1$, and the relation matrix (also called pseudo-covariance) $C = 0$. The formulae of these parameters are given by:

\begin{equation}
\label{eqcbn3}
\begin{aligned}
&\mu = \mathbb{E}[\tilde{x}]&\\
&\Gamma = \mathbb{E}[(\tilde{x}-\mu)(\tilde{x}-\mu)^{*}] = V_{rr} + V_{ii} + i (V_{ir} - V_{ri} )&\\
&C = \mathbb{E}[(\tilde{x}-\mu)(\tilde{x}-\mu)] = V_{rr} - V_{ii} + i (V_{ir} + V_{ri} )&
\end{aligned}
\end{equation}

The next step in complex-valued BN is to scale and shift the input distrubtion to a desired mean and variance, with the help of $\gamma$ and $\beta$, respectively. $\gamma$ is now a $\mathrm{2} \mathrm{x} \mathrm{2}$ positive-semidefinite matrix with three tunable parameters and it is given by:

\begin{equation}
\gamma = \begin{pmatrix} \gamma_{rr} & \gamma_{ri}\\ \gamma_{ri} & \gamma_{ii} \end{pmatrix} 
\label{eqcbn4}
\end{equation}

$\beta$ is a complex parameter with real and imaginary learnable components. The final step of complex-valued BN follows the same equation as of real-valued BN, given by:

 \begin{equation}
y_{k} = \mathrm{BN}_{\gamma,\beta}(x_{k}) = \gamma\hat{x}_{k} + \beta
\label{eqcbn5}
\end{equation}

%SKIPPING: DECORRELATION EFFECT, IMPLEMENTATION DETAILS....
 \subsection{Activation function}\label{ssec:act_fn}
 In the absence of non-linear activation functions, the output of a neural network would essentially be a linear combination of the input, however deep the network is. Non-linear activation functions are introduced in order to add non-linearity to the network so that functions of a higher degree than 1 can be learned, thus giving them the title of universal function approximators. In a CNN, activation function is applied element-wise to the output of the convolution (or BN, when it is used) and they act as to decide whether or not this output should be taken into account by its connection, and if so, what should be the nature of it. For $\mathrm{R}$-CNNs, some common types of activation functions include Sigmoid, Hyperbolic Tangent function (tanh), and the most popular, Recitified Linear Unit (ReLU).
 
 %They also serve as to provide a degree of robustness to data variability and %a computationally efficient representation, courtesy of sparsity induction.  
 
  \subsubsection{Real-Valued Activation Functions}
   \subsubsection{ReLU}
   
Popularized by \cite{krizhevsky2012imagenet}, Rectified Linear Units (ReLU) \cite{nair2010rectified} remains the most commonly used activation function in deep learning/CNN architectures for the community. ReLU is given by:

\begin{equation}
\mathrm{ReLU}(x) =  
\begin{cases} 
x , &\textrm{for} \ x\ge 0\\
0 , &\textrm{for} \ x < 0
\end{cases}
\end{equation}
 
%benefits of ReLU
%other 
 
 Other variants of ReLU include Leaky ReLU (LReLU), Parametric ReLU (PReLU), Randomized ReLU (RReLU), and Exponential Linear Unit (ELU), as shown in Figure \ref{ractfn}. 
  
 
  \begin{figure}[htb]
 	\centering
 	\epsfxsize=14cm
 	{\epsfbox{relu.png}}
 	\caption{Variants of ReLU activation function}}
 	\label{ractfn}
 \end{figure}
  
  
  
 \subsubsection{Complex-Valued Activation Functions}\label{cvaf}
 
 Real-valued activation functions in $\mathbb{R}$-CNNs have to strictly comply with the requirement of differentiability for the chain rule, the backbone of the Gradient Descent optimization algorithm, to work. Since $\mathbb{C}$-CNNs also make use of the 
 same optimization algorithm, it is only natural to think that the complex activation functions must also be differentiable at any point in their domain i.e. they should be Holomorphic. However, Louiville theorem presents us with a realization: the only complex functions that can be  Holomorphic are constant functions. This begs us to question if the condition of differentiability can be relaxed while still being able to optimize the network.
 We find a way to circumvent this problem by ignoring the requirement of complex differentiability, finding solace in the fact that as long as the complex function is differentiable with respect to its real part and its imaginary part, it should provide sufficiency in its ability to perform back-propagation, as demonstrated in \cite{trabelsi2018deep}, \cite{hansch2010complex}, \cite{polsarzhang2017complex}. The chain-rule for complex function differentiation is given in Section \ref{cchainrule}. However, we must note that since Holomorphic functions obey CR equations, they can be  beneficial in granting computational efficiency to our process as we would only need to compute half the number of partial derivatives \cite{sarroff2015learning}.
 
 %cooridnate dependency????
     
 Complex-valued activation functions can be divided into two types: Split-complex type and Transcendental type.
 Split-complex type functions deals with real and imaginary parts (Real-Imaginary subtype) or magnitude and phase (Magnitude-phase subtype) independently but identically. Hence, they can be thought to work similar to real-valued networks with double the number of dimensions. The Split-complex activation functions are said to be appropriate when we can assume that our data contains cartesian symmetry when using Real-imaginary type, rotational symmtery when using Magnitude-phase type. Split-complex functions are Holomorphic but unbounded.
 
 Transcendental type is the one which are bounded almost everywhere, with well-defined first order derivatives. They do have singularities but it is suggested by Kima and Adali (2003) \cite{kim2003approximation} that with proper weight initialization and regularization schemes, this type of activation functions will not hamper the process of backpropagation.
 
 %suitability of the two types functions???
 
 
 %" Instead, we construct the dynamics of learning and self-organization on the basis of meaningful partial derivatives in complex domain with deliberation on the properties of  input and output information "
 


%where $b$ is a learnable parameter which dictates the offset for the deadzone around the origin $0$. modReLU is an element-wise nonlinearity, where the element is muted(assigned a value of zero in both real and imaginary channels) if it lies in the dead-zone, or passed if it is beyond. Chiheb $et \ al.$ \cite{trabelsi2018deep} also make use of this non-linearity in their analysis of $\mathbb{R}$-CNNs and $\mathbb{C}$-CNNs.


\subsubsection{$\mathbb{C}$ReLU}
$\mathbb{C}$ReLU is Real-Imaginary type activation function introduced by Chiheb $et \ al.$ (2018) \cite{trabelsi2018deep}, which satisfies the CR equations only in the first and the third quadrant. The formula is given as follows:

\begin{equation}
\label{eqcrelu}
\mathbb{C}\mathrm{ReLU}(z) = \mathrm{ReLU}(\Re(z))+i\mathrm{ReLU}(\Im(z))
\end{equation}

 
 
 \subsubsection{$z$ReLU}
 Proposed by Guberman (2106) \cite{Guberman}, $z$ReLU does not strictly fall in any of the two categories described above. It follows the CR equations on all points on the complex domain except for the points along the real and imaginary axes of the complex plane. It is defined as follows:
  \begin{equation}
  \label{eqzrelu}
 z\mathrm{ReLU}(z) 
 {}=\begin{cases} z , &\mathrm{for} \ \theta_{z} \in [0,\pi/2] + b\ge 0 \\ 0 , &\textrm{otherwise} \end{cases}
 \end{equation}

%SPLIT COMPLEX SOFTMAX????



\subsubsection{tanh($z$)}
A Transcendental type, tanh($z$) is a complex- hyperbolic tangent function suggested by Kim and Adali 2003 \cite{kim2003approximation}.
This function gives us a periodic singularity at every
$(1/2 + n)\pi i$, $n \in \mathbb{N}$, as shown in Fig. \ref{tanhz}. However, even in the presence of singularities, this activation function can be used to train the network with some success, although the possibility of hitting the singularity remains.

\begin{equation}
\mathrm{tanh}(z) = \mathrm{tanh}(x+iy) = 
{}=\begin{cases} \Re(\mathrm{tanh}(z)) = \frac{\mathrm{sinh}(2x)}{\mathrm{cosh}(2x)+\mathrm{cosh}(2y)} \\ \Im(\mathrm{tanh}(z)) = \frac{sinh(2x)}{\mathrm{cosh}(2x)+\mathrm{cosh}(2y)} \end{cases}
\end{equation}


 \subsection{Weight initialization}
 Weight initialization can help in speeding up the convergence of the gradient descent algorithm, and combats the problem of vanishing and exploding gradients, especially in the absence of BN. 
 
 
 \subsubsection{Real-valued weight initialization}\label{rvwi}
 In the context of $\mathbb{R}$-CNNs, two weight initialization techniques are popular among the community, which are explained below. In these two techniques, the first step concerns with choosing choosing a random distribution function whose defining parameters can be adjusted according to their benefits.
 \paragraph{Glorot and Bengio (2010) criterion}
 This type of initialization demands that the variance of the weight distibution should be as follows:

  \begin{equation}\label{gbwi}
\mathrm{Var}(W) = 2/(n_{in}+n_{out})
\end{equation}
 
 The constraint ensures that the variances of the input, output and the gradients are the same. They go ahead to pick the weights from a Gaussian distribution with zero mean and variance as given in equation\ref{gbwi}. This initialization has been proved to be useful owing to the fact that it can detect the scale of initialization based on the number of inputs and outputs, keeping the signal in a reasonable limit through many layers \cite{recent_advances} \cite{glorot2010understanding}.
 
 
 \paragraph{He $et \ al.$ (2015b) criterion}
 He $et \ al.$ (2105b) proposed an initialization criterion specifically accounts for ReLU non-linearity function, which grants the ability to train extremely deep networks \cite{recent_advances} \cite{he2015delving}. 
 
 
 
 \begin{equation}\label{hewi}
 \mathrm{Var}(W) = 2/n_{in}
 \end{equation}
 \subsubsection{Complex-valued weight initialization}\label{cwi}
 Chiheb $et \ al.$ (2018) introduced two methods of complex weight initialization, which incorporate the two real-valued $\mathbb{R}$-CNN weight initialization techniques given above.
 
 They start with dealing with the problem of defining the variance of complex weights. The standard definition of variance of complex variables goes as follows:
 
 \begin{equation}\label{vardef}
 \mathrm{Var}(W) = \mathbb{E}[WW^{*}]-(\mathbb{E}[W])^2 = \mathbb{E}[|W|^{2}]-(\mathbb{E}[W])^2
 \end{equation}
 
 
 When $W$ is a symmetrically distributed around 0, equation \ref{vardef} reduces to $\mathrm{Var}(W) = \mathbb{E}[|W|^{2}]$. In order to calculate this variance, we adopt a route that involves Rayleigh distribution. We know that the variance of the magnitude of complex weights, $\mathrm{Var}(|W|)$, follows a Rayleigh distribution, given below:
 
 \begin{equation}\label{varmagrayleighdef}
 \mathrm{Var}(|W|) = \mathbb{E}[|W||W|^{*}]-(\mathbb{E}[|W|])^2 = \mathbb{E}[|W|^{2}]-(\mathbb{E}[|W|])^2
 \end{equation}
 
 Bu substitution, we reach the following formulation for the variance of the weights:
 
 \begin{equation}\label{varw2}
 \mathrm{Var}(|W|) = \mathrm{Var}(W)-(\mathrm{E}[|W|])^2 , and  
 \mathrm{Var}(W) = \mathrm{Var}(|W|) + (\mathrm{E}[|W|])^2
 \end{equation}
  
 The expectation and variance of magnitude of complex weights can be computer using the definitions given by the Rayleigh distribution, which can be characterized by just one parameter, mode ($\sigma$). By substitution, we can finally define the variance of the complex weights.
 
 \begin{equation}\label{ray1}
 \mathrm{E}(|W|) = \sigma\sqrt{\frac{\pi}{2}},\\  
 \mathrm{Var}(|W|) = \frac{4-\pi}{2}\sigma^{2}
 \end{equation}
 
 
 \begin{equation}\label{ray2}
 \mathrm{Var}(W) = 2\sigma^2
 \end{equation}
 
 Now that the variance is defined,  we can choose either Glorot and Bengio criterion or the He. $2t \ al.$ (2015b) criterion to equate to define the value of $\sigma$. In the former criterion, we get $\sigma = 1/\sqrt{n_{in}+n_{out}}$, and in the latter, $\sigma = 1/\sqrt{n_{in}}$. Having the appropriate $\sigma$, we can pick the magnitude of our weights from teh Rayleigh distribution. However, as the variance of $W$ depends only on the magnitude of the weights and not their phase,  we have the liberty of choosing phase. In their implementation, phase is chosen from a uniform distribution of -$\pi$ to $\pi$.
 
 Finally, we can use equation \ref{lastcwi} to form the real and imaginary parts of the weights.
 
 \begin{equation}\label{lastcwi}
 W = |W|e^{i\theta} = \Re\{W\}+i\Im\{W\}
 \end{equation}
 
 
 %\subsubsection{Complex-Valued Weight Initialization}
 
 
 %\paragraph{Complex Weight Initialization}
 %\paragraph{Complex Independent Weight Initialization}
 
 \subsection{Pooling layer}
 The sequence of layers in a typical CNN is as given below: 
 \begin{equation}
 Convolutional \ Layer\longrightarrow BN \longrightarrow Activation \ Function \longrightarrow  Pooling \ Layer
 \end{equation}
 
 Pooling layer operates patch-wise on the input to extract a statistical summary of the patch that replaces the pixel corresponding to the central pixel of the patch in the output. This results in merging semantically similar features into one. One of the benefit lies in the reduced size of the inputs and weights for the next layers which reduces their computational burden. Another benefit is slight translational invariance of feature, since even if a feature occur at a slightly different areas of the feature map, the pooling layer would result in the same output as if it hadn't. 
 %Pooling also helps in combating the problem of overfitting 

 \subsubsection{Pooling in $\mathbb{R}$-CNNs}
 \paragraph{Max pooling}
 Max pooling is a patch-wise operation whose output is the maximum of all the values in the patch. Its formula is given by:
 
 \begin{equation}
 y = \underset{i,j \in patch}{max} x_{i,j}
 \end{equation}
 where $x$ is a real number, and $i,j$ are the indices of the patch.
 
 \paragraph{Average pooling}
 Average pooling is a patch-wise operation whose output is the maximum of all the values in the patch. Its formula is given by:

 \begin{equation}
 y = \bigg(\underset{i,j \in patch}{\sum} x_{i,j} \bigg)/ sizeOfPatch
 \end{equation}
 
 where $x$ is a real number, and $i,j$ are the indices of the patch.
 \subsubsection{Pooling in $\mathbb{C}$-CNNs}
% Pooling in $\mathbb{C}$-CNNs aims to achieve the same effects as those in $\mathbb{R}$-CNNs but we are encountered by some choices and restrictions.   
 \paragraph{Max-by-mag pooling}
 Maximum operator is not defined for a Complex Field since it is not an ordered field. Hence, we cannot compare two complex numbers wholly. Guberman (2016) \cite{Guberman} proposed the max-by-magnitude pooling, where the basis of comparison is the magnitude. In $\mathbb{C}$-CNNs, we are to decide what the statistical summary of the output should be based on. Knowing that choosing either the real part or the imaginary part would be wrong as it will mean that one is more important than the other, magnitude is chosen to provide the comparison. However, the output is itself a complex number courtesy of the $argmax$ operator, which allows the remainder of the network to remain in complex form. The formulation is given by:
 

 
\begin{equation}
y = \underset{i,j \in patch}{argmax} \ |z_{i,j}|
\end{equation}
 where $z$ is a complex number, and $i,j$ are the indices of the patch.
\paragraph{Average pooling}
Averaging operator is trivially generalized to the complex domain. As th  
 
 
  \begin{equation}
 y = \bigg(\underset{i,j \in patch}{\sum} z_{i,j} \bigg)/ sizeOfPatch
 \end{equation}
 
 where $z$ is a complex number, and $i,j$ are the indices of the patch.
 
 \subsection{Fully-connected Layer}
 Fully-connected (FC) layer converts the CNN into a traditional neural network in that the 2D grid structure of the feature maps of the layers are vectorized and, unlike a CNN, each input is connected to each output through a weight. A single FC layer can exist at the end of the network, or a number of FC layers can exist until the output. If and when the FC layer becomes the output layer, it serves to provide the class-wise probability in a classification problem. If and when an FC layer exists as a hidden layer, an affine transformation is performed on the input: weight multiplication and bias addition, followed by application of the activation function.
 
  \subsubsection{FC layer in $\mathbb{R}$-CNNs}
 In $\mathbb{R}$-CNNs, the FC layer at the output usually employ the Softmax function which squashes the input between 0 and 1 so that it gives non-negative class-wise probability distribution over the classes. It is given by:
 
 
 
 \begin{equation} p_j^{(i)}=\frac{e^{z_j^i}}{\underset{l=1...K}{\sum}e^{z_l^{(i)}}} 
 \end{equation}
 
 where $z_j^{(i)}$ is the activation of the FC layer. 

\subsubsection{FC layer in $\mathbb{C}$-CNNs}
In $\mathbb{C}$-CNNs, the FC layer which act as hidden layers, same as in $\mathbb{R}$-CNNs, undergo an affine transformation (weight multiplication and bias addition) followed by an acitvation function. As complex numbers cannot be ordered, meaning that one cannot tell if a complex number is greater than the other, we are compelled to convert the complex numbers into real numbers. For this purpose, we find three ways of implementation in the literature: 
\begin{enumerate}
\item Choosing to project the complex numbers to real domain by magnitude projection and feeding them to a real-valued FC layer, as proposed by Guberman (2016) \cite{Guberman}.
\item Treating the real and imaginary components as two independent real-valued channels feeding into two output FC layers, as in Zhang $et \ al.$ (2017) \cite{polsarzhang2017complex}.
\item Ignoring the distinction between real and imaginary components by feeding them directly to a real-valued output FC layer, as in Chiheb $et \ al.$ (2018) \cite{trabelsi2018deep} 
\end{enumerate}


	 \subsection{Loss function}
 Loss functions take on two roles in a convolutional neural network - regularization, and providing a measure of comparison between the prediction and the ground truth label. Regularization refers to the idea that the loss function can help enforce a specific quality of the learning in a neural network i.e. L2 regularization prefers that the result obtained is not a consequence of a little number of peaky weight vectors, but a large number of diffused weight vectors. In a classification problem solved with the help of neural networks, we intend to minimize the error, i.e. the difference between the prediction and the ground truth label, using an optimization technique, most common of which is Stochastic Gradient Descent (discussed in Section \ref{optbp}). Different loss functions will give different errors for the same prediction, and thus have a considerable effect on the performance of the model. One of the strict requirements is that loss functions are differentiable so that error gradients can be calculated.
 
 
 
 \subsubsection{Loss functions in $\mathbb{R}$-CNNs}
 In the classification problem, $\mathbb{C}$-CNNs employ a variety of loss functions including Mean Squared Error (MSE), Hinge Loss, Cross Entropy etc. One of the most common loss functions used for multi-class classification, Categorical Cross Entropy (negative), is defined as:
 
  \begin{equation}\label{rloss}
  L_{y^{'}}(y) = - \sum_{i=1} y_{i}^{'} \log(y_{i})
 \end{equation}
 where $y_{i}$ is the predicted probability for class $i$ and $y_{i}^{'}$ is the ground truth label of the class. Cross Entropy, true to the nature of a loss function, shows a trend of increase as the difference between the predicted and the ground truth class label increases.
 
 
 
 \subsubsection{Loss functions in $\mathbb{C}$-CNNs}
 The loss function is preceded by an FC layer which gives us probability scores of the output. Examples of loss functions used in $\mathbb{C}$-CNNs when the real and imaginary channels are compared with the respective real and imanginary components of the complex-valued label include Complex Quadratic Error Function, Complex FOurth Power Error Function, Complex Cauchy Error Function, Complex Log-Cosh Error Function, Adapted Least-Squares Error, among others, as found in \cite{hansch2009classification}\cite{polsarzhang2017complex}\cite{hansch2010complex}. 
 In the other cases when the loss function compares two real numbers (prediction and label), any $\mathbb{R}$-CNN based loss function can be used e.g. Categorical Cross Entropy.    
 
 \subsection{Optimization through backpropagation}\label{optbp}
 In the supervised learning problem involving CNNs, the output layer of the system produces a vector of scores, each denote the probability of each hypothesis class $i$, where $i \in [1,P]$. Ideally, the $k^{th}$ entry of the vector, $O(k)$, should larger than the others, corresponding to and correctly identifying the input data class. However, the system needs to go through a training or optimization, in which the parameters or weights of the system are tuned such that they give us correct predictions of the result.
 
 The most common procedure to train CNNs is known as Stochastic Gradient Descent(SGD). This learning algorithm involves computing a gradient vector,  with the help of the chain-rule for derivatives, that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount \cite{nature}. SGD updates the parameters $\theta$ of the objective $L(\theta)$ as $\theta_{t+1} = \theta_{t} - \eta_{t}\nabla_{\theta}E[L(\theta_{t})]$,
 where $E[L(\theta_{t})$ is the average of $L(\theta)$ over a sub-set of the input datset called minibatch, and $\eta$ is the learning rate. Hence, differentiability of the loss function and the activation functions is a strict condition in the case of $\mathbb{R}$-CNNs. However, as discussed in section \ref{cvaf}, such strict requirement can be relaxed for the case of $\mathbb{C}$-CNNs, as substantiated by the literature. 
    
 





