\chapter{Methodology} \label{chap:methodology}
 
This section details the methodology adopted to investigate the applicability of $\mathbb{C}$-CNNs to complex-natured data classification. 

\section{Approach}

In our experimentation, we aim to answer the following question:\\
 
 \textit{Can $\mathbb{C}$-$\mathrm{CNNs}$ outperform $\mathbb{R}$-$\mathrm{CNNs}$ in a complex-valued data classification problem, given that phase of the complex-natured input data contains, as well, discriminatory information?}\\
 
 The aims of our experimentation are:
 \begin{itemize}
 	\item To establish the importance of various complex-valued activation functions in $\mathbb{C}$-CNNs
 	\item To determine if $\mathbb{C}$-CNNs can better utilize phase information to produce better results than $\mathbb{R}$-CNNs
 	%\item To assess if $\mathbb{C}$-CNNs offer better generalization capability as compared to $\mathbb{R}$-CNNs
 \end{itemize}
 
 
 We employ two types of datasets: synthetically created toy data set which we call MNIST+P (Section \ref{data-mnistp}), and real-world Radar datasets (Section \ref{data-radar}), to feed into comparable $\mathbb{R}$-CNN and $\mathbb{C}$-CNN architectures in a bid to experimentally determine the winner in the complex-valued data classification problem. MNIST+P dataset is created to test if additional information of phase can help increase classification accuracy of $\mathbb{C}$-CNNs more than that of $\mathbb{R}$-CNNs, under the assumption that the complex-nature data would be better exploited by $\mathbb{C}$-CNNs. In this case, we are sure that phase does contain meaningful information which can be leveraged to increase accuracy. The real-world radar datasets are used for experimental verification of the deductions made from the results of MNIST+P dataset experiments, and to test applicability of $\mathbb{C}$-CNNs an FMCW radar data. 
 
 The architecture of $\mathbb{R}$-CNN and $\mathbb{C}$-CNN employed in this task is discussed in Section \ref{archdesc}.  All the experiments are conducted using Keras \cite{chollet2015keras} with Theano \cite{theano} backend.
 
 
 \section{Datasets}\label{datasets}
 
 
 \subsection{MNIST+P Dataset}\label{data-mnistp}
 The MNIST database of handwritten digits (10 classes) has a training set of 60,000 examples, and a test set of 10,000 examples, where each image is of dimensions 28x28. It is a commonly used benchmark for supervised learning algorithm performance. In the MNIST+P dataset created for our experimentation, we consider that an image of MNIST dataset represents the magnitude of a complex number. The magnitude in each image is scaled such that the black part of the image corresponds to the value of 50, and the white part to 200. The phase for each image class (0,1,2,3,4,5,6,7,8,9) is picked from uniform distribution whose range is determined class-wise: The range for class 1 representing the digit 0 is [0,$\pi$/10], that for class 2 representing the digit 1 is [$\pi$/10,2$\pi$/10],...., that for class 10 representing digit 9 is [9$\pi$/10,$\pi$]. 
 
 \begin{figure}[htb]
 	\centering
 	\epsfxsize=7cm
 	{\epsfbox{MnistExamples}}\caption{Examples of images in MNIST Dataset \cite{mnistimage}}
 	\label{fig:blocks}
 \end{figure}
 
  
  
 \section{Architecture -- $\mathbb{R}$-CNNs \& $\mathbb{C}$-CNNs}\label{archdesc}
 The architecture employed for our experiments is a simplified variant of the one used by Chiheb $et \ al.$ (2018) \cite{trabelsi2018deep} for the problem of real-valued image classification. Our network contains three stages, each containing one residual block with 2 convolutional layers (3x3 filter size), 2 activation functions, and 2 BN layers in the order shown in Figure \ref{fig:blocks}. The skip connection of the first block differs from that of the second and the third block. The skip connection of the first block simply adds the input of the block to the output of the other thread of the block. However, the last two blocks perform a projection operation at their output: they convolve the input of the block with a 1x1 convolution operation with the same number of filters used throughout the stage, and concatenate it with the output of the other thread (with 2 conv, 2 BN, 2 activation layers) in a way that the distinction between real and imaginary parts remains. At the end of each stage, the number of filters are doubled. The architecture is illustrated in Figure \ref{fig:architecture}.
 
 
 %We hypothesize that since, $\mathbb{C}$-CNNs will be able to perform better than $\mathbb{R}$-CNNs, given the compelling reasons described in Section \ref{sect:moti}.

  \begin{figure}[htb]
 	\centering
 	\epsfxsize=5cm
 	{\epsfbox{1}}\ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \
 	\epsfxsize=6.75cm
 	{\epsfbox{23}}\ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \
 		\epsfxsize=5cm
 	{\epsfbox{ARCH}}
 	\caption{Architecture of the first residual block in stage 1 (left), and that in stages 2 and 3 (right); General architecture of both $\mathbb{R}$-CNN and $\mathbb{C}$-CNN (bottom)}
 	\label{fig:architecture}
 \end{figure}
 

 
 