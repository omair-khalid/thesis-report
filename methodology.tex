\chapter{Methodology} \label{chap:methodology}
 
This section details the methodology adopted to investigate the applicability of $\mathbb{C}$-CNNs to complex-natured data. In order to do so, we aim to answer the following question:\\
 
 \textit{Can $\mathbb{C}$-$\mathrm{CNNs}$ outperform $\mathbb{R}$-$\mathrm{CNNs}$ in a classification problem, given that phase of the complex-natured input data contains, as well, discriminatory information?}\\
 
 We employ two datasets: synthetically created toy data set, which we call MNIST-P (Section \ref{data-mnistp}), and real-world Radar dataset (Section \ref{data-radar}), to feed into comparable $\mathbb{R}$-CNN and $\mathbb{C}$-CNN networks in a bid to experimentally determine the winner in the classification problem. The architecture of $\mathbb{R}$-CNN and $\mathbb{C}$-CNN employed in this task is discussed in the following section.
 

 
 
 
 
 briefly mention the challenges of building a complex valued neural networks. 
 
 

   
 \section{Practical implementation of complex numbers}
 Consider a complex number $z=a+ib$ with the real component $a$ and the imaginary component $b$. In $\mathbb{R}$-CNNs, the filter bank or the feature maps of a layer exist in 3 dimensions, where two of them denote the size of the filter or feature map in 2D, and the third dimension indicates how many there are. If we have $N$ feature maps (where $N$ is divisible by 2), the first $N/2$ feature maps would be dedicated to the real components ($a$) and the last $N/2$ feature maps would be dedicated to the imaginary components ($b$). In this manner, the imaginary feature map corresponding to the real feature map on index one of the 3rd dimension would like on the $\frac{N}{2} +1$ position.
 
 IMAGE REQUIRED
 
 
 
 
 \subsection{Architecture -- $\mathbb{R}$-CNNs \& $\mathbb{C}$-CNNs}
 The architecture employed for our experiments is a simplified variant of the one used by Chiheb $et \ al.$ (2018) \cite{trabelsi2018deep} for the problem of real-valued image classification. It contains three residual blocks containing 2 convolutional layers (3x3 filter size), 2 ReLU activation functions, and two BN layers in the order shown in Figure \ref{arch}. The skip connection of the first block differs from that of the second and the third block. The skip connection of the first block simply adds the input of the block to the output of the other thread of the block. However, the last two blocks perform a projection operation at their output that concatenates the output of the last residual block with the output of a 1x1 convolution applied on it with the same number of filters used throughout the stage and subsample by a factor of 2. While the $\mathbb{C}$-CNNs perform complex convolutions, complex BN, complex average-pooling, the $\mathbb{R}$-CNNs employs the real-valued counter parts. The weight initialization of $\mathbb{R}$-CNNs are done following the Glorot and Bengio criterion (2010) \cite{glorot2010understanding}, while that of $\mathbb{C}$-CNNs follow the complex-valued counterpart of the same, as described in section \ref{cwi}. The last layer of both the network types contain a FC layer employing the Softmax function as the activation function, and the loss function is categorical-cross entropy.
 
 Discuss design choices again as well.
 
  WHY BRO WHY DID YOU CHOOSE AVERAGE POOLING?
 AHAAA!  We don't wanna have to deal with a activation function whose values are only positive!
 
 
 
 
 
 We hypothesize that since, $\mathbb{C}$-CNNs will be able to perform better than $\mathbb{R}$-CNNs, given the compelling reasons described in Section \ref{sect:moti}.
 
 
 
 
 