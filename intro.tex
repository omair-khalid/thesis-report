\chapter{Introduction} \label{chap:intro}

\section{Motivation} \label{sect:thefirst}

Complex-valued Convolutional Neural Networks ($\mathbb{C}$-CNNs) are that class of Convolutional Neural Networks (CNNs) that incorporate complex number representations in their operation. This may mean that either data, internal parameters (weights), or both data and weights are complex in nature. We motivate our problem by first describing why CNNs are a suitable choice for image classification, followed by why complex numbers should be incorporated into CNNs for complex-natured data.

CNNs have emerged to be one of the most powerful tool in computer vision for the tasks including, but not limited to, image classification, and segmentation. CNNs make use of the compositional hierarchy of features present in natural images, i.e. low-level elements coming together to form higher-level representations, while enjoying the benefits of sparse connectivity, weight-sharing, and invariance to translations (further explained in \ref{cnn41})  They gained tremendous popularity after they were used by Krizhevsky $et \ al.$ (2012) \cite{krizhevsky2012imagenet} used them to win the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 by achieving record-breaking results. Recent years have seen major advancements in their performance which includes techniques like Batch Normalization \cite{bnIoffeS15}, Drop-Out regularization, different architecture designs (AlexNet, ResNet) etc.

Complex numbers lend themselves to model neurons that exhibit the properties of a human brain. Reichert and Serre (2013) \cite{reichert2013neuronal} incorporated complex-valued neurons to represent the functioning of a biologically inspired neuron in a deep learning network that exhibits dynamic binding: depending on the input of the network, the phases of particular neurons adding up or canceling each other, so that as a whole, the network provides the correct output. Hence, the usage of complex numbers can help us achieve a closer representation of natural phenomena as compared to that of only real-valued numbers.

In literature, one of the advantages of $\mathbb{C}$-NNs can provide better guard against overfitting phenomenon, leading to better generalization. This idea is also put forward by Hirose and Yoshida (2012) \cite{hirose2012complex} who explain that the benefit of complex-valued neural networks ($\mathbb{C}$-NNs) compared to real-valued neural networks ($\mathbb{R}$-NNs) lies in the restriction in flexibility in learning brought by the nature of multiplication, as it entails rotation and scaling, not just scaling as in real-valued case. This grants the network relief from "ineffective degrees of freedom" that leads to better generalization. Guberman (2016) \cite{Guberman} also reports better generalization for $\mathbb{C}$-CNNs as compared to $\mathbb{R}$-CNNs. Hence, if better generalization is desired, $\mathbb{C}$-CNNs might have the answer.

The recent successes of $\mathbb{C}$-CNNs with Polarimetric Synthetic Aperture (SAR) data in various classification tasks (\cite{polsarzhang2017complex}, \cite{hansch2010complex}, \cite{wilmanski2016complex}), audio transcription, and speech spectrum prediction \cite{trabelsi2018deep}, against $\mathbb{R}$-CNNs presents us with a strong case of their potential advantages when the input data is complex-natured itself. Although the research of $\mathbb{C}$-NNs is not new, the recent advancements in CNNs and general deep learning architectures offer us an opportunity to further investigate how complex number representations can be useful in this new light. 

%According to Bruna $et. \ al$ (2015) \cite{bruna2015theoretical}, a simple complex-valued network, having repeated blocks of complex convolution, taking absolute of result, and local averaging, can be regarded data-driven multi-scale windows absolute spectra. However, their complex-valued neural network definition does not incorporate non-linearites and thus the latest implementations of Complex-valued neural networks cannot be equated to the ones defined by the authors.

%In short, in CVNNs, we can reduce ineffective degree of freedom in learning or self-organization to achieve better generalization characteristics. If we know a priori
%that the objective quantities include “phase” and/or “amplitude,” we can reduce possibly harmful portion of the freedom by employing a complex-valued neural network, resulting in a more meaningful generalization characteristics\\

%physical meaning of radar magnitude and phase? discernibility lies in them? WHAT IS MEANT BY COMPLEX DATA? MAG AND PHASE RELATION TO PHYSICAL QUANTITY? magnitude alone cannot grant us what they can do together.
%historical uses

%Why should complex data invite the use of complex representations?
%


\section{Problem definition} \label{sect:thefirst}
We aim to experiment and analyse if $\mathbb{C}$-CNNs can perfom better than $\mathbb{R}$-CNNs when the input is complex-natured data.




\section{Document structure} \label{sect:thefirst}


